---
description: 
globs: 
alwaysApply: true
---
# ChatSphere Debugging Guide

## Overview
This document outlines debugging standards and best practices for ChatSphere.

## Related Universal Guides
- @globalrules/ai-debugging.md
- @globalrules/ai-error-handling.md

## Related Technology Rules
- @technology rules/django.md
- @technology rules/vuejs.md
- @technology rules/api.md

## Debugging Architecture

### 1. Logging Configuration

```python
# backend/config/logging.py
import logging.config
import os
from datetime import datetime

LOGGING = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'verbose': {
            'format': '{levelname} {asctime} {module} {process:d} {thread:d} {message}',
            'style': '{',
        },
        'simple': {
            'format': '{levelname} {message}',
            'style': '{',
        },
    },
    'filters': {
        'require_debug_true': {
            '()': 'django.utils.log.RequireDebugTrue',
        },
    },
    'handlers': {
        'console': {
            'level': 'INFO',
            'filters': ['require_debug_true'],
            'class': 'logging.StreamHandler',
            'formatter': 'simple'
        },
        'file': {
            'level': 'INFO',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': os.path.join('logs', f'chatsphere-{datetime.now():%Y-%m-%d}.log'),
            'maxBytes': 1024 * 1024 * 5,  # 5 MB
            'backupCount': 5,
            'formatter': 'verbose',
        },
        'error_file': {
            'level': 'ERROR',
            'class': 'logging.handlers.RotatingFileHandler',
            'filename': os.path.join('logs', f'chatsphere-error-{datetime.now():%Y-%m-%d}.log'),
            'maxBytes': 1024 * 1024 * 5,  # 5 MB
            'backupCount': 5,
            'formatter': 'verbose',
        }
    },
    'loggers': {
        'django': {
            'handlers': ['console', 'file'],
            'level': 'INFO',
            'propagate': True,
        },
        'chatsphere': {
            'handlers': ['console', 'file', 'error_file'],
            'level': 'INFO',
            'propagate': True,
        },
        'chatsphere.bots': {
            'handlers': ['console', 'file', 'error_file'],
            'level': 'INFO',
            'propagate': False,
        },
        'chatsphere.conversations': {
            'handlers': ['console', 'file', 'error_file'],
            'level': 'INFO',
            'propagate': False,
        }
    }
}

logging.config.dictConfig(LOGGING)
```

### 2. Error Tracking Setup

```python
# backend/config/sentry.py
import sentry_sdk
from sentry_sdk.integrations.django import DjangoIntegration
from sentry_sdk.integrations.redis import RedisIntegration
from sentry_sdk.integrations.celery import CeleryIntegration

def init_sentry(dsn: str, environment: str) -> None:
    """
    Initialize Sentry error tracking.
    
    Args:
        dsn: Sentry DSN
        environment: Environment name (development/staging/production)
    """
    sentry_sdk.init(
        dsn=dsn,
        environment=environment,
        integrations=[
            DjangoIntegration(),
            RedisIntegration(),
            CeleryIntegration(),
        ],
        traces_sample_rate=1.0,
        send_default_pii=False,
        before_send=before_send,
    )

def before_send(event: dict, hint: dict) -> dict:
    """
    Filter sensitive information before sending to Sentry.
    """
    if 'request' in event:
        # Remove sensitive headers
        if 'headers' in event['request']:
            headers = event['request']['headers']
            headers.pop('Authorization', None)
            headers.pop('Cookie', None)
        
        # Remove sensitive POST data
        if 'data' in event['request']:
            data = event['request']['data']
            if 'password' in data:
                data['password'] = '[Filtered]'
            if 'token' in data:
                data['token'] = '[Filtered]'
    
    return event
```

### 3. Frontend Error Handling

```typescript
// frontend/src/utils/error-handler.ts
import { AxiosError } from 'axios'
import * as Sentry from '@sentry/vue'

interface APIError {
  code: string
  message: string
  details?: Record<string, any>
}

export class ErrorHandler {
  /**
   * Handle API errors and format them for display.
   */
  static handleAPIError(error: AxiosError<APIError>): string {
    // Log to Sentry
    Sentry.captureException(error)
    
    if (error.response?.data) {
      const apiError = error.response.data
      return `${apiError.message} (${apiError.code})`
    }
    
    if (error.response?.status === 404) {
      return 'Resource not found'
    }
    
    if (error.response?.status === 401) {
      return 'Please log in to continue'
    }
    
    if (error.response?.status === 403) {
      return 'You do not have permission to perform this action'
    }
    
    return 'An unexpected error occurred'
  }
  
  /**
   * Handle AI-specific errors.
   */
  static handleAIError(error: Error): string {
    // Log to Sentry with AI context
    Sentry.withScope(scope => {
      scope.setTag('error_type', 'ai')
      Sentry.captureException(error)
    })
    
    if (error.message.includes('token_limit')) {
      return 'Message is too long for processing'
    }
    
    if (error.message.includes('content_filter')) {
      return 'Message contains inappropriate content'
    }
    
    return 'AI processing error occurred'
  }
}
```

### 4. Debug Middleware

```python
# backend/core/middleware.py
import time
import logging
from typing import Callable
from django.http import HttpRequest, HttpResponse

logger = logging.getLogger('chatsphere')

class DebugMiddleware:
    """
    Middleware for debugging request/response cycle.
    Only active when DEBUG=True.
    """
    
    def __init__(self, get_response: Callable):
        self.get_response = get_response
    
    def __call__(self, request: HttpRequest) -> HttpResponse:
        if not settings.DEBUG:
            return self.get_response(request)
        
        # Start timer
        start_time = time.time()
        
        # Log request
        logger.debug(f"Request: {request.method} {request.path}")
        logger.debug(f"Headers: {dict(request.headers)}")
        
        if request.body:
            logger.debug(f"Body: {request.body.decode()}")
        
        # Process request
        response = self.get_response(request)
        
        # Calculate duration
        duration = time.time() - start_time
        
        # Log response
        logger.debug(f"Response Status: {response.status_code}")
        logger.debug(f"Duration: {duration:.2f}s")
        
        if hasattr(response, 'data'):
            logger.debug(f"Response Data: {response.data}")
        
        return response
```

### 5. AI Debug Tools

```python
# backend/bots/debug.py
from typing import Dict, List, Optional
import logging
from dataclasses import dataclass

logger = logging.getLogger('chatsphere.bots')

@dataclass
class AIDebugContext:
    """Context for debugging AI responses."""
    prompt: str
    response: str
    tokens_used: int
    duration_ms: float
    temperature: float
    model: str
    error: Optional[str] = None

class AIDebugger:
    """Tools for debugging AI interactions."""
    
    def __init__(self):
        self.history: List[AIDebugContext] = []
    
    def log_interaction(
        self,
        prompt: str,
        response: str,
        metadata: Dict
    ) -> None:
        """Log an AI interaction for debugging."""
        context = AIDebugContext(
            prompt=prompt,
            response=response,
            tokens_used=metadata.get('tokens_used', 0),
            duration_ms=metadata.get('duration_ms', 0),
            temperature=metadata.get('temperature', 0),
            model=metadata.get('model', 'unknown')
        )
        
        self.history.append(context)
        
        logger.info(
            "AI Interaction",
            extra={
                'prompt_length': len(prompt),
                'response_length': len(response),
                'tokens_used': context.tokens_used,
                'duration_ms': context.duration_ms,
                'model': context.model
            }
        )
    
    def analyze_response(
        self,
        response: str,
        expected_format: Optional[Dict] = None
    ) -> List[str]:
        """
        Analyze AI response for potential issues.
        Returns list of warnings/issues found.
        """
        issues = []
        
        # Check response length
        if len(response) < 10:
            issues.append("Response too short")
        
        # Check for common error patterns
        if "error" in response.lower():
            issues.append("Response contains error message")
        
        # Validate JSON format if expected
        if expected_format:
            try:
                response_json = json.loads(response)
                for key in expected_format:
                    if key not in response_json:
                        issues.append(f"Missing required key: {key}")
            except json.JSONDecodeError:
                issues.append("Invalid JSON format")
        
        return issues
    
    def get_performance_stats(self) -> Dict:
        """Get performance statistics from interaction history."""
        if not self.history:
            return {}
        
        total_tokens = sum(c.tokens_used for c in self.history)
        total_duration = sum(c.duration_ms for c in self.history)
        avg_tokens = total_tokens / len(self.history)
        avg_duration = total_duration / len(self.history)
        
        return {
            'total_interactions': len(self.history),
            'total_tokens': total_tokens,
            'total_duration_ms': total_duration,
            'avg_tokens_per_interaction': avg_tokens,
            'avg_duration_ms': avg_duration
        }
```

## Debugging Procedures

### 1. Backend Debugging

#### Django Debug Toolbar Setup

```python
# backend/config/settings/local.py
INSTALLED_APPS += ['debug_toolbar']
MIDDLEWARE += ['debug_toolbar.middleware.DebugToolbarMiddleware']

DEBUG_TOOLBAR_CONFIG = {
    'SHOW_TOOLBAR_CALLBACK': lambda request: True,
    'EXTRA_SIGNALS': True,
    'HIDE_DJANGO_SQL': False,
    'ENABLE_STACKTRACES': True,
}

DEBUG_TOOLBAR_PANELS = [
    'debug_toolbar.panels.versions.VersionsPanel',
    'debug_toolbar.panels.timer.TimerPanel',
    'debug_toolbar.panels.settings.SettingsPanel',
    'debug_toolbar.panels.headers.HeadersPanel',
    'debug_toolbar.panels.request.RequestPanel',
    'debug_toolbar.panels.sql.SQLPanel',
    'debug_toolbar.panels.staticfiles.StaticFilesPanel',
    'debug_toolbar.panels.templates.TemplatesPanel',
    'debug_toolbar.panels.cache.CachePanel',
    'debug_toolbar.panels.signals.SignalsPanel',
    'debug_toolbar.panels.logging.LoggingPanel',
]
```

#### Database Query Debugging

```python
# backend/core/debug.py
from django.db import connection
from typing import List, Callable
from functools import wraps

def log_queries(func: Callable) -> Callable:
    """
    Decorator to log database queries.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        initial_queries = len(connection.queries)
        
        result = func(*args, **kwargs)
        
        final_queries = len(connection.queries)
        num_queries = final_queries - initial_queries
        
        if num_queries > 0:
            logger.debug(f"{func.__name__} executed {num_queries} queries:")
            for query in connection.queries[initial_queries:final_queries]:
                logger.debug(f"SQL: {query['sql']}")
                logger.debug(f"Time: {query['time']}s")
        
        return result
    return wrapper
```

### 2. Frontend Debugging

#### Vue DevTools Setup

```typescript
// frontend/src/main.ts
import { createApp } from 'vue'
import App from './App.vue'

const app = createApp(App)

if (process.env.NODE_ENV === 'development') {
  app.config.performance = true
  app.config.devtools = true
}
```

#### Component Debugging

```typescript
// frontend/src/components/debug/ComponentDebug.vue
<script setup lang="ts">
import { ref, onMounted, onUpdated } from 'vue'

const props = defineProps<{
  componentName: string
}>()

const updates = ref(0)
const mountTime = ref(0)
const updateTimes = ref<number[]>([])

onMounted(() => {
  mountTime.value = performance.now()
  console.log(`${props.componentName} mounted in ${mountTime.value}ms`)
})

onUpdated(() => {
  updates.value++
  const updateTime = performance.now()
  updateTimes.value.push(updateTime)
  console.log(`${props.componentName} updated (${updates.value})`)
})
</script>

<template>
  <div class="debug-info" v-if="process.env.NODE_ENV === 'development'">
    <details>
      <summary>Debug Info</summary>
      <p>Mount Time: {{ mountTime }}ms</p>
      <p>Updates: {{ updates }}</p>
      <p>Last Update: {{ updateTimes[updateTimes.length - 1] }}ms</p>
    </details>
  </div>
</template>
```

### 3. AI Debugging

#### Response Validation

```python
# backend/bots/validators.py
from typing import Dict, List, Optional
import re

class AIResponseValidator:
    """
    Validates AI responses for quality and safety.
    """
    
    def __init__(self):
        self.safety_patterns = [
            r'(password|token|key|secret)',
            r'(ssh|ftp|telnet)',
            r'(\d{1,3}\.){3}\d{1,3}'  # IP addresses
        ]
    
    def validate_response(
        self,
        response: str,
        context: Dict
    ) -> List[str]:
        """
        Validate an AI response.
        Returns list of validation issues.
        """
        issues = []
        
        # Check for sensitive information
        for pattern in self.safety_patterns:
            if re.search(pattern, response, re.I):
                issues.append(f"Response contains sensitive pattern: {pattern}")
        
        # Check response relevance
        relevance_score = self.check_relevance(response, context)
        if relevance_score < 0.5:
            issues.append("Response may not be relevant to context")
        
        # Check response format
        if context.get('expected_format'):
            format_issues = self.validate_format(
                response,
                context['expected_format']
            )
            issues.extend(format_issues)
        
        return issues
    
    def check_relevance(
        self,
        response: str,
        context: Dict
    ) -> float:
        """
        Check response relevance to context.
        Returns score between 0 and 1.
        """
        # Implementation depends on specific relevance checking logic
        # Could use embedding similarity, keyword matching, etc.
        return 1.0
    
    def validate_format(
        self,
        response: str,
        expected_format: Dict
    ) -> List[str]:
        """
        Validate response format against expected schema.
        """
        issues = []
        
        try:
            response_json = json.loads(response)
            
            for key, value_type in expected_format.items():
                if key not in response_json:
                    issues.append(f"Missing required key: {key}")
                elif not isinstance(response_json[key], value_type):
                    issues.append(
                        f"Invalid type for {key}: "
                        f"expected {value_type.__name__}, "
                        f"got {type(response_json[key]).__name__}"
                    )
        except json.JSONDecodeError:
            issues.append("Invalid JSON format")
        
        return issues
```

## Implementation Guidelines

### 1. Debugging Process Flow

1. Identify Issue
   - Error reports
   - User feedback
   - Monitoring alerts
   - Performance issues

2. Gather Information
   - Error messages
   - Stack traces
   - Logs
   - Request/response data
   - Database queries
   - Performance metrics

3. Reproduce Issue
   - Local environment
   - Development environment
   - With minimal test case

4. Analyze Root Cause
   - Code review
   - Log analysis
   - Debug tools
   - Performance profiling

5. Implement Fix
   - Write tests
   - Fix code
   - Verify fix
   - Document solution

6. Verify Solution
   - Test in development
   - Deploy to staging
   - Monitor metrics
   - User verification

### 2. Debug Environment Setup

1. Local Setup
   ```bash
   # Backend
   export DJANGO_DEBUG=True
   export DEBUG_TOOLBAR=True
   export LOG_LEVEL=DEBUG
   
   # Frontend
   export VUE_APP_DEBUG=true
   export VUE_APP_API_MOCK=true
   ```

2. Development Tools
   - Django Debug Toolbar
   - Vue DevTools
   - Browser DevTools
   - Database GUI
   - Log viewers
   - Performance profilers

### 3. Logging Best Practices

1. Log Levels
   - ERROR: Errors that need immediate attention
   - WARNING: Issues that might need attention
   - INFO: Normal operation events
   - DEBUG: Detailed debugging information

2. Log Format
   ```python
   logger.info(
       "Event description",
       extra={
           'user_id': user.id,
           'action': action,
           'duration_ms': duration,
           'status': status
       }
   )
   ```

## Implementation Checklist

- [ ] Set up logging configuration
- [ ] Configure error tracking
- [ ] Install debugging tools
- [ ] Create debug middleware
- [ ] Implement AI debugging
- [ ] Add performance monitoring
- [ ] Write debugging documentation
- [ ] Train team on procedures

## Common Pitfalls

1. Insufficient Logging
   - Missing context
   - Too verbose/noisy
   - Poor formatting

2. Security Issues
   - Logging sensitive data
   - Debug mode in production
   - Exposed error details

3. Performance Impact
   - Heavy logging
   - Debug tools overhead
   - Unnecessary tracking

4. Poor Organization
   - Inconsistent logging
   - Missing documentation
   - Unclear procedures

## Additional Resources

1. [Django Debug Toolbar](mdc:https:/django-debug-toolbar.readthedocs.io)
2. [Vue DevTools Guide](mdc:https:/devtools.vuejs.org)
3. [Python Debugging Guide](mdc:https:/docs.python.org/3/library/pdb.html)
4. [Sentry Documentation](mdc:https:/docs.sentry.io) 