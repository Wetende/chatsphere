 # Step 1: Plan QA Testing Scope
- Define the scope of QA testing in a spec file (e.g., `qa-plan.md`).
- Include functional, performance, and usability requirements to test.
- Use Cursor AI to validate the plan and suggest additional test cases.

# Step 2: Generate QA Test Cases
- Use Cursor AI to create detailed test cases based on the spec.
  - For functional tests, cover all user flows (e.g., login, data submission).
  - For performance tests, define load thresholds (e.g., 100 concurrent users).
  - For usability, outline steps to verify UI/UX consistency.
- Save test cases in `qa-tests.md` or a dedicated testing tool.

# Step 3: Execute QA Tests
- Manually or automatically run the test cases.
- Use Cursor AI to generate scripts for automation (e.g., Selenium for UI, JMeter for load testing).
- Record results, including pass/fail status and any defects.

# Step 4: Iterate Based on Results
- Follow the instructions in `@ai-tests` to refine tests and fix issues.
- Use Cursor AI to analyze failed tests and suggest code fixes or test adjustments.
- Re-run tests until all critical issues are resolved.

# Step 5: Document and Share Findings
- Summarize QA results in `qa-report.md`, including metrics like defect density or test coverage.
- Use Cursor AI to format the report and highlight key insights for stakeholders.